{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display\n",
    "from recurrent import gen_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By setting the first and second dimensions to None, we allow\n",
    "# arbitrary minibatch sizes with arbitrary sequence lengths.\n",
    "# The number of feature dimensions is 2, as described above.\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, 2))\n",
    "# This input will be used to provide the network with masks.\n",
    "# Masks are expected to be matrices of shape (n_batch, n_time_steps);\n",
    "# both of these dimensions are variable for us so we will use\n",
    "# an input shape of (None, None)\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All gates have initializers for the input-to-gate and hidden state-to-gate\n",
    "# weight matrices, the cell-to-gate weight vector, the bias vector, and the nonlinearity.\n",
    "# The convention is that gates use the standard sigmoid nonlinearity,\n",
    "# which is the default for the Gate class.\n",
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    " W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    " b=lasagne.init.Constant(0.))\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    " W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    " # Setting W_cell to None denotes that no cell connection will be used.\n",
    " W_cell=None, b=lasagne.init.Constant(0.),\n",
    " # By convention, the cell nonlinearity is tanh in an LSTM.\n",
    " nonlinearity=lasagne.nonlinearities.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our LSTM will have 10 hidden/cell units\n",
    "N_HIDDEN = 10\n",
    "l_lstm = lasagne.layers.recurrent.LSTMLayer(\n",
    " l_in, N_HIDDEN,\n",
    " # We need to specify a separate input for masks\n",
    " mask_input=l_mask,\n",
    " # Here, we supply the gate parameters for each gate\n",
    " ingate=gate_parameters, forgetgate=gate_parameters,\n",
    " cell=cell_parameters, outgate=gate_parameters,\n",
    " # We'll learn the initialization and use gradient clipping\n",
    " learn_init=True, grad_clipping=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The \"backwards\" layer is the same as the first,\n",
    "# except that the backwards argument is set to True.\n",
    "l_lstm_back = lasagne.layers.recurrent.LSTMLayer(\n",
    " l_in, N_HIDDEN, ingate=gate_parameters,\n",
    " mask_input=l_mask, forgetgate=gate_parameters,\n",
    " cell=cell_parameters, outgate=gate_parameters,\n",
    " learn_init=True, grad_clipping=100., backwards=True)\n",
    "# We'll combine the forward and backward layer output by summing.\n",
    "# Merge layers take in lists of layers to merge as input.\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_lstm, l_lstm_back])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, retrieve symbolic variables for the input shape\n",
    "n_batch, n_time_steps, n_features = l_in.input_var.shape\n",
    "# Now, squash the n_batch and n_time_steps dimensions\n",
    "l_reshape = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "# Now, we can apply feed-forward layers as usual.\n",
    "# We want the network to predict a single value, the sum, so we'll use a single unit.\n",
    "l_dense = lasagne.layers.DenseLayer(\n",
    " l_reshape, num_units=1, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "# Now, the shape will be n_batch*n_timesteps, 1. We can then reshape to\n",
    "# n_batch, n_timesteps to get a single value for each timstep from each sequence\n",
    "l_out = lasagne.layers.ReshapeLayer(l_dense, (n_batch, n_time_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation cost = 0.046245187521\n",
      "Epoch 2 validation cost = 0.0390758290887\n",
      "Epoch 3 validation cost = 0.0487103126943\n",
      "Epoch 4 validation cost = 0.0274621322751\n",
      "Epoch 5 validation cost = 0.0209543332458\n",
      "Epoch 6 validation cost = 0.0191643983126\n",
      "Epoch 7 validation cost = 0.0116968825459\n",
      "Epoch 8 validation cost = 0.0152641050518\n",
      "Epoch 9 validation cost = 0.00905874744058\n",
      "Epoch 10 validation cost = 0.00728162936866\n"
     ]
    }
   ],
   "source": [
    "# Symbolic variable for the target network output.\n",
    "# It will be of shape n_batch, because there's only 1 target value per sequence.\n",
    "target_values = T.vector('target_output')\n",
    "# This matrix will tell the network the length of each sequences.\n",
    "# The actual values will be supplied by the gen_data function.\n",
    "mask = T.matrix('mask')\n",
    "# lasagne.layers.get_output produces an expression for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_out)\n",
    "# The value we care about is the final value produced for each sequence\n",
    "# so we simply slice it out.\n",
    "predicted_values = network_output[:, -1]\n",
    "# Our cost will be mean-squared error\n",
    "cost = T.mean((predicted_values - target_values)**2)\n",
    "# Retrieve all parameters from the network\n",
    "all_params = lasagne.layers.get_all_params(l_out)\n",
    "# Compute adam updates for training\n",
    "updates = lasagne.updates.adam(cost, all_params)\n",
    "# Theano functions for training and computing cost\n",
    "train = theano.function(\n",
    " [l_in.input_var, target_values, l_mask.input_var],\n",
    " cost, updates=updates)\n",
    "compute_cost = theano.function(\n",
    " [l_in.input_var, target_values, l_mask.input_var], cost)\n",
    "# We'll use this \"validation set\" to periodically check progress\n",
    "X_val, y_val, mask_val = gen_data()\n",
    "# We'll train the network with 10 epochs of 100 minibatches each\n",
    "NUM_EPOCHS = 10\n",
    "EPOCH_SIZE = 100\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for _ in range(EPOCH_SIZE):\n",
    "        X, y, m = gen_data()\n",
    "        train(X, y, m)\n",
    "    cost_val = compute_cost(X_val, y_val, mask_val)\n",
    "    print(\"Epoch {} validation cost = {}\".format(epoch + 1, cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
